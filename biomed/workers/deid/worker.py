import concurrent.futures
import json
import os
import shutil
import subprocess
import tempfile
from typing import List, Optional

from text2phenotype.annotations.file_helpers import (
    TextCoordinateSet,
    Annotation,
    AnnotationSet,
)
from text2phenotype.common.featureset_annotations import MachineAnnotation
from text2phenotype.constants.clinical_summary import AnnotationLabelEnum
from text2phenotype.common.log import operations_logger
from text2phenotype.common.redaction import (
    create_redacted_img,
)
from text2phenotype.constants.common import FileExtensions, deserialize_enum
from text2phenotype.constants.deid import DeidGroupings
from text2phenotype.constants.features import PHILabel, FeatureType
from text2phenotype.tasks.rmq_worker import RMQConsumerTaskWorker
from text2phenotype.tasks.task_enums import TaskEnum, WorkType, TaskOperation
from text2phenotype.tasks.task_info import DeidTaskInfo, OCRTaskInfo
from text2phenotype.services.storage import (
    StorageProvidersEnum,
    get_storage_service,
)

from biomed.biomed_env import BiomedEnv
from biomed.deid.global_redaction_helper_functions import redact_text, global_redact_and_filter
from biomed.summary.text_to_summary import get_page_indices


class DeidTaskWorker(RMQConsumerTaskWorker):
    QUEUE_NAME = BiomedEnv.DEID_TASKS_QUEUE.value
    TASK_TYPE = TaskEnum.deid
    RESULTS_FILE_EXTENSION = DeidTaskInfo.RESULTS_FILE_EXTENSION
    GLOBAL_REDACT_FILTER_EXT = DeidTaskInfo.GLOBAL_REDACT_FILTER_EXT
    WORK_TYPE = WorkType.document
    NAME = 'DeidTaskWorker'
    ROOT_PATH = BiomedEnv.root_dir

    def get_redacted_txt_filename(self):
        return (f'{self.work_task.document_id}.{self.RESULTS_FILE_EXTENSION}.'
                f'{FileExtensions.TXT.value}')

    def get_redacted_phi_tokens_filename(self):
        return (f'{self.work_task.document_id}.{self.GLOBAL_REDACT_FILTER_EXT}.'
                f'{FileExtensions.JSON.value}')

    def get_redacted_pdf_filename(self):
        return (f'{self.work_task.document_id}.{self.RESULTS_FILE_EXTENSION}.'
                f'{FileExtensions.PDF.value}')

    def do_work(self) -> DeidTaskInfo:
        task_result: DeidTaskInfo = self.init_task_result()

        # get the source text
        source_text: str = self.download_object_str(self.work_task.text_file_key)
        if self.get_job_task().deid_filter in DeidGroupings.__members__:
            phi_category_filter = DeidGroupings[self.get_job_task().deid_filter]
        else:
            phi_category_filter = DeidGroupings.SAFE_HARBOR

        operations_logger.info(f"Using {phi_category_filter} to filter deid tokens for redaction")

        # get phi tokens from storage.
        phi_tokens = self.get_json_results_from_storage(
            TaskEnum.phi_tokens)

        # get demographics from storage
        demographics = self.get_json_results_from_storage(TaskEnum.demographics)
        # get annotation from storage
        annotations = {k: v for k, v in self.get_json_results_from_storage(TaskEnum.annotate).items()
                       if k in ['token', 'range', FeatureType.date_comprehension.name]}

        phi_res = global_redact_and_filter(
            demographic_json=demographics,
            machine_annotation=MachineAnnotation(json_dict_input=annotations),
            phi_token_json=phi_tokens,
            phi_categories_to_include=phi_category_filter.value
        )
        # upload thet intermediate phi json
        task_result.redacted_phi_tokens_file_key = self.upload_intermediate_result(
            json.dumps(phi_res.to_json(task_operation=TaskOperation.phi_tokens)).encode('utf-8'),
            object_name=self.get_redacted_phi_tokens_filename()
        )

        # redact text
        result_text: str = redact_text(phi_res, source_text)
        task_result.redacted_txt_file_key = self.upload_intermediate_result(
            result_text.encode('utf-8'), object_name=self.get_redacted_txt_filename())

        if TaskEnum.ocr not in self.work_task.task_statuses:
            # Text file
            task_result.results_file_key = task_result.redacted_txt_file_key

            return task_result
        else:
            # PDF file
            ocr_task: OCRTaskInfo = self.work_task.task_statuses[TaskEnum.ocr]

            text_coord_set: TextCoordinateSet = TextCoordinateSet.from_storage(
                directory_filename=ocr_task.text_coords_directory_file_key,
                lines_filename=ocr_task.text_coords_lines_file_key,
                storage_client=self.storage_client)

            page_numbers = get_page_indices(source_text)
            text_coord_set.update_from_page_ranges(page_numbers=page_numbers)

            # AnnotationSet() should be pre-generated by the reassembler for all Biomed tasks probably
            annotation_set: AnnotationSet = self.create_annotations(
                text_coord_set,
                phi_res.to_json()[PHILabel.get_category_label().persistent_label])

            task_result.results_file_key = self.generate_redacted_file_pdf(
                annotations=annotation_set,
                text_coords=text_coord_set,
                png_file_keys=ocr_task.png_pages)

            return task_result

    @staticmethod
    def create_annotations(text_coord_set: TextCoordinateSet, phi_tokens: List):
        annotation_set = AnnotationSet()
        for ann in phi_tokens:
            ann_range = ann['range']
            ann_label = ann['label']
            ann_text = ann['text']

            text_coords = text_coord_set.find_coords(ann_range[0], ann_range[1])
            coord_uuids = []

            first = True
            for tc in text_coords:
                if first:
                    start = tc.document_index_first
                    stop = tc.document_index_last
                    coord_uuids = [tc.uuid]

                    line_start = tc.line
                    line_stop = tc.line

                    first = False
                else:
                    stop = tc.document_index_last
                    line_stop = tc.line

                    coord_uuids.append(tc.uuid)

            entry = Annotation(category_label=AnnotationLabelEnum.PHI.value,
                               label=ann_label,
                               text_range=[start, stop],
                               text=ann_text,
                               coord_uuids=coord_uuids,
                               line_start=line_start,
                               line_stop=line_stop,
                               )
            annotation_set.directory[entry.uuid] = entry

        return annotation_set

    def generate_redacted_file_pdf(
            self,
            annotations: AnnotationSet,
            text_coords: TextCoordinateSet,
            png_file_keys: List[str],
            tid: str = None) -> str:
        operations_logger.info(f'Generating redacted PDF', tid=tid)

        work_dir = tempfile.mkdtemp()
        try:
            # set up working directories
            png_out_dir = tempfile.mkdtemp(dir=work_dir)
            pdf_out_dir = tempfile.mkdtemp(dir=work_dir)
            redacted_filepath = os.path.join(work_dir, 'redacted.pdf')

            # create pages dir for coords to redact
            pages_directory = {}
            for ann in annotations.entries:

                if ann.category_label == AnnotationLabelEnum.PHI.value:
                    for tc_id in ann.coord_uuids:
                        tc = text_coords[tc_id]
                        pg_list = pages_directory.setdefault(tc.page, [])
                        pg_list.append(tc)

            provider = None
            options = None
            if BiomedEnv.DEFAULT_STORAGE_SERVICE.value == StorageProvidersEnum.S3.name:
                credentials = self.storage_client.session.get_credentials()
                provider = StorageProvidersEnum.S3
                options = dict(aws_access_key_id=credentials.access_key,
                               aws_secret_access_key=credentials.secret_key,
                               bucket_name=BiomedEnv.STORAGE_CONTAINER_NAME.value,
                               region_name=BiomedEnv.AWS_REGION_NAME.value,
                               aws_session_token=credentials.token)

            inputs = []
            operations_logger.debug(f'Looping through PNGs', tid=tid)
            for pg, png_url in png_file_keys.items():
                inputs.append((pages_directory, int(pg), png_url, png_out_dir,
                               pdf_out_dir, provider, options))

            with concurrent.futures.ProcessPoolExecutor(max_workers=4) as executor:
                for page_args in inputs:
                    executor.submit(self.redact_page, *page_args)

            # now concatenate all pdfs
            cmd = ["pdfunite", os.path.join(pdf_out_dir, '*'), redacted_filepath]
            operations_logger.debug(f'Running command: {" ".join(cmd)}', tid=tid)
            subprocess.run(' '.join(cmd), shell=True, check=True)

            with open(redacted_filepath, 'rb') as f:
                contents = f.read()

            return self.upload_intermediate_result(content=contents,
                                                   object_name=self.get_redacted_pdf_filename())
        finally:
            shutil.rmtree(work_dir)

    @staticmethod
    def redact_page(pages_directory, page, png_url, png_out_dir, pdf_out_dir, provider, options):
        storage_client = get_storage_service(provider=provider, options=options)
        # Download PNG file
        base_name = os.path.basename(png_url)
        png_path = f'{png_out_dir}/{base_name}'

        content = storage_client.get_content(png_url)
        with open(png_path, 'wb') as f:
            f.write(content)

        png_fnam, _ = os.path.splitext(base_name)
        redacted_fnam = f'{png_fnam}_redacted'
        redacted_filepath = os.path.join(png_out_dir, f"{redacted_fnam}.png")

        redact_coords = pages_directory.get(page, [])

        create_redacted_img(png_path, redact_coords, redacted_filepath)

        cmd = ['convert', redacted_filepath, os.path.join(pdf_out_dir, f'{redacted_fnam}.pdf')]
        operations_logger.debug(f"Converting redacted PNG to PDF: {' '.join(cmd)}")
        subprocess.run(cmd, check=True)

    def stop(self, timeout: Optional[float] = None) -> None:
        return super().stop(timeout=30.0)
