import os
from typing import List, Tuple, Union
from io import StringIO

import pandas as pd
import seaborn as sns
from matplotlib import pyplot as plt

from medal.common.aws import get_matching_s3_keys, get_object_str, get_s3_client

from medal.common.log import operations_logger
from medal.common import common
from biomed.models.testing_reports import ConfusionPrecisionMisclassReport
from biomed import RESULTS_PATH

# global defaults
BUCKET_NAME = "ciox-195338640440-biomed-data"
S3_MODEL_TEST_PATH = "models/test"
S3_MODEL_TRAIN_PATH = "models/train"


def report_text_to_score_melt(model_name: str, report_text: str):
    """
    Return long form (melted) DataFrame with scores
    Expects doc string that is generated by
        ConfusionPrecisionMisclassReport.precision_recall_report
    and formatted by
        ConfusionPrecisionMisclassReport.scipy_report_str

    Each row will have one score associated with it, differentiated
    by class_label and score_type (both strings)

    :param model_name: Name used to distinguish models, does not need to be job_id
    :param report_text: raw text from report_weighted.txt file
    :return: pd.DataFrame
    """
    df = ConfusionPrecisionMisclassReport.parse_classification_text_to_df(report_text)
    df.insert(0, "model_name", model_name)
    # strip off leading/trailing whitespace
    df["class_label"] = df["class_label"].str.strip()
    score_melt = pd.melt(
        df,
        id_vars=["model_name", "class_label"],
        value_vars=["precision", "recall", "f1-score"],
        var_name="score_type",
        value_name="score",
    )
    return score_melt


def score_files_to_melt_df(scores_filenames: List[str], short_model_names: List[str]):
    """
    Read in score files and the abbreviated model names, returning a melted dataframe with scores

    :param scores_filenames: filenames for the target scores files
    :param short_model_names: should match order of scores_file_names
    :return: pd.DataFrane
        Concatenated rows for all score filenames, in melted format
    """
    score_df_list = []
    for short_name, fn in zip(short_model_names, scores_filenames):
        scores_text = common.read_text(fn)
        operations_logger.info(f"Loaded report text from: {fn}")
        if not scores_text:
            operations_logger.warning(f"file {fn} had no text in the report!")
        score_melt = report_text_to_score_melt(short_name, scores_text)
        score_df_list.append(score_melt)
    score_melt_df = pd.concat(score_df_list, ignore_index=True)

    return score_melt_df


def plot_model_perf_by_class_label(
        score_melt_df, class_label, sort_by: Union[str, None] = "f1-score",
        report_name=None, target_model=None, ymin: float = 0.7, unpin_ylim: bool = False
):
    """
    Create grouped bar plot with the precision, recall, and f1 scores for a list of models

    :param score_melt_df: pd.DataFrame
        dataframe in long form (melted), with scores by model, class_label, and score_type
    :param class_label:
        The target label to display
        Use "avg/total" to display the average
    :param sort_by: Union[str, None]
        score_type to sort by, default to f1 score
        if None, don't sort the model order, use the order from config
    :param report_name: str
        Report name to use in plot title
    :param target_model: Optional[str]
        model_name to highlight score, marked by selected `sort_by` score_type
    :param ymin: float, minimum value for the bar plot if scores are higher than bar_min-0.1
    :param unpin_ylim: bool, if true, let the graph choose the correct ylim

    :return: matplotlib.Figure
    """
    report_name = report_name or "Model Performance"
    if class_label not in score_melt_df.class_label.unique():
        raise ValueError(
            f"Target class label '{class_label}' was not found in the dataframe, "
            f"found {score_melt_df.class_label.unique()}")
    score_label_df = score_melt_df[score_melt_df["class_label"] == class_label]
    if sort_by:
        model_order = (
            score_label_df[score_label_df["score_type"] == sort_by]
            .sort_values(by="score")["model_name"]
            .values.tolist()
        )
    else:
        model_order = score_label_df.model_name.unique().tolist()

    g = sns.catplot(
        data=score_label_df,
        kind="bar",
        x="model_name",
        y="score",
        hue="score_type",
        order=model_order,
        # palette="dark",
        alpha=0.9,
        legend=False,
        height=6,
        aspect=1.2,
    )
    g.despine(left=True)
    g.set_axis_labels("", "score")
    if sort_by:
        g.ax.legend(loc="lower right", facecolor="white", framealpha=0.7)
    else:
        g.ax.legend(facecolor="white", framealpha=0.7)

    # plot red horizontal line for the model to beat
    if target_model:
        if not score_label_df["model_name"].isin([target_model]).any():
            raise ValueError(f"Could not find target_model '{target_model}' in models")
        target_score = score_label_df[
            (score_label_df["model_name"] == target_model)
            & (score_label_df["score_type"] == (sort_by or "f1-score"))
        ]["score"].values[0]
        plt.axhline(target_score, linestyle=":", color="r", label="target f1 score")
    ax = plt.gca()
    if not unpin_ylim:
        ax.set_ylim((min(ymin, score_label_df["score"].min() - 0.1), 1.02))
    plt.xticks(rotation=-30, ha="left")
    plt.xlabel("features")
    label = "avg" if class_label.replace(" ", "") == "avg/total" else class_label
    plt.title(f"{report_name}\nlabel: {label}", weight="bold")
    plt.tight_layout()
    return plt.gcf()


def plot_input_size_vs_epoch_time(base_support_df):
    """
    Plot the average epoch duration versus the input data size

    :param base_support_df: pd.DataFrame
    :return: matplotlib.Figure
    """
    plt.figure()
    g = sns.lmplot(
        data=base_support_df,
        x="train_matrix_size",
        y="epoch_durations_sec",
        hue="model_type_name",
        logx=True,
        fit_reg=True,
        aspect=1.2,
        legend=False,
    )
    g.despine(left=True)
    g.ax.legend(loc="upper left", facecolor="white", framealpha=0.7)
    plt.xlabel("train input matrix size")
    plt.ylabel("avg epoch duration (s)")
    plt.title(f"Train duration by input size")
    plt.ylim((0.0, plt.ylim()[1]))
    plt.tight_layout()
    return plt.gcf()

def get_s3_model_report_str(s3_client, bucket: str, model_key_path: str, report_prefix: str) -> str:
    """
    Pull the model report string from the .txt file with `report_prefix`

    :param s3_client: s3.client.Client
    :param bucket: str, target bucket name
    :param model_key_path: str,
     the s3 prefix up to and including the model job_id
    :param report_prefix: str
        the report name prefix, eg "
    :return: str
    """
    prefix_match = os.path.join(model_key_path, report_prefix)
    matching_keys = list(get_matching_s3_keys(bucket, prefix=prefix_match))
    if not matching_keys:
        raise ValueError(f"Didn't find a matching object in s3: {prefix_match}")
    elif len(matching_keys) >= 2:
        raise ValueError(f"Too many matches with given prefix: {matching_keys}")
    report_str = get_object_str(s3_client, bucket, matching_keys[0])
    return report_str


def get_s3_model_metadata_str(s3_client, bucket: str, job_id: str) -> str:
    prefix_match = os.path.join(S3_MODEL_TEST_PATH, job_id)
    matching_keys = list(get_matching_s3_keys(bucket, prefix=prefix_match))

    metadata_filename = [key for key in matching_keys if ".h5.metadata.json" in key]
    if not metadata_filename:
        raise ValueError(f"Didn't find a matching object in s3: {metadata_filename}")
    if len(metadata_filename) >= 2:
        metadata_filename = sorted(metadata_filename)
        operations_logger.warning(f"Too many matches with given prefix, returning most recent: {metadata_filename}")
    report_str = get_object_str(s3_client, bucket, metadata_filename[-1])
    return report_str

def model_collection(
        model_list: List[Tuple[str, str]],
        report_prefix: str,
        bucket=BUCKET_NAME,
        file_type='txt'
) -> pd.DataFrame:
    """
    Load report info from s3, create a combined comparison

    :param model_list: List[Tuple[str,str]]
        A list of tuples for the target model job_ids and display name
    :param report_prefix: str
        S3 prefix to use for model report locations
    :param bucket: str
        bucket name containing target reports
    :param file_type: str
        governs how the file will be processed (csv for misclassification)
    :return: pd.DataFrame
        Combined comparison dataframe
    """
    s3_client = get_s3_client()
    score_df_list = []
    # validate file_type
    if file_type not in ('csv', 'txt'):
        operations_logger.info(
            f"Invalid file type: {file_type}, only accepts csv or txt")
    for model_job_id, model_name in model_list:
        model_s3_key = os.path.join(S3_MODEL_TEST_PATH, model_job_id)
        try:
            report_data = get_s3_model_report_str(s3_client, bucket, model_s3_key, report_prefix=report_prefix)
        except Exception as e:
            # wasnt able to access bucket, try locally?
            operations_logger.info(
                f"Error getting data from s3: {e}, looking for report locally: "
                f"{os.path.basename(model_s3_key)}, {report_prefix}")
            local_result_path = os.path.join(RESULTS_PATH, os.path.basename(model_s3_key))
            target_file_path = common.get_file_list(local_result_path, f"{report_prefix}.{file_type}")[0]
            if file_type=='txt':
                report_data = common.read_text(target_file_path)
            elif file_type=='csv':
                report_data = common.read_csv(target_file_path)
        if file_type=='txt':
            score_melt = report_text_to_score_melt(model_name, report_data)
        elif file_type == 'csv':
            report_csv = StringIO(report_data)
            score_melt = pd.read_csv(report_csv)
            score_melt['model_source'] = model_name
        score_df_list.append(score_melt)
    score_melt_df = pd.concat(score_df_list, ignore_index=True)
    return score_melt_df

def model_collection_precision_recall_f1_report(
        model_list: List[Tuple[str, str]],
        report_prefix: str,
        bucket=BUCKET_NAME
) -> pd.DataFrame:
    """
    Load report data from S3, collect in melted dataframe, and create figure

    :param model_list: List[Tuple[str,str]]
        A list of tuples for the target model job_ids and display name
    :param report_prefix: str
        S3 prefix to use for model report locations
    :param bucket: str
        bucket name containing target reports
    :return: pd.DataFrame
        The melted score table used to create figure
    """
    s3_client = get_s3_client()
    score_df_list = []
    for model_job_id, model_name in model_list:
        model_s3_key = os.path.join(S3_MODEL_TEST_PATH, model_job_id)
        try:
            report_data = get_s3_model_report_str(s3_client, bucket, model_s3_key, report_prefix=report_prefix)
        except Exception as e:
            # wasnt able to access bucket, try locally?
            operations_logger.info(
                f"Error getting data from s3: {e}, looking for report locally: "
                f"{os.path.basename(model_s3_key)}, {report_prefix}")
            local_result_path = os.path.join(RESULTS_PATH, os.path.basename(model_s3_key))
            target_file_path = common.get_file_list(local_result_path, f"{report_prefix}.txt")[0]
            report_data = common.read_text(target_file_path)
        score_melt = report_text_to_score_melt(model_name, report_data)
        score_df_list.append(score_melt)
    score_melt_df = pd.concat(score_df_list, ignore_index=True)

    return score_melt_df
